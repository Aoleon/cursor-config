<!-- 
Context: reinforcement-learning, Q-learning, rewards, penalties, exploration-exploitation, strategy-optimization
Priority: P1
Auto-load: when optimizing agent behavior with advanced RL techniques, Q-learning, exploration-exploitation
Dependencies: core.md, reinforcement-learning.md, self-evolution-engine.md, learning-memory.md
Score: 70
-->

# Apprentissage par Renforcement Avanc√© - Saxium

**Objectif:** Impl√©menter un syst√®me d'apprentissage par renforcement avanc√© avec Q-learning, exploration-exploitation optimis√©e et convergence vers strat√©gies optimales bas√© sur m√©triques r√©elles.

**R√©f√©rence:** [Cursor Rules Documentation](https://docs.cursor.com/context/rules)  
**Version:** 2.0.0  
**Derni√®re mise √† jour:** 2025-01-29

## üéØ Principe Fondamental

**IMP√âRATIF:** L'agent DOIT utiliser un syst√®me d'apprentissage par renforcement avanc√© avec Q-learning pour optimiser la s√©lection de techniques, √©quilibrer exploration vs exploitation et converger vers des strat√©gies optimales.

**B√©n√©fices:**
- ‚úÖ Syst√®me r√©compenses/p√©nalit√©s bas√© m√©triques r√©elles
- ‚úÖ Apprentissage Q-learning pour s√©lection techniques
- ‚úÖ Exploration vs exploitation optimis√©e
- ‚úÖ Convergence vers strat√©gies optimales
- ‚úÖ Adaptation automatique selon contexte

**R√©f√©rence:** `@.cursor/rules/reinforcement-learning.md` - Apprentissage par renforcement de base  
**R√©f√©rence:** `@.cursor/rules/self-evolution-engine.md` - Moteur d'auto-√©volution  
**R√©f√©rence:** `@.cursor/rules/learning-memory.md` - M√©moire persistante

## üìã Syst√®me de R√©compenses/P√©nalit√©s Avanc√©

### M√©triques R√©elles

**TOUJOURS:**
- ‚úÖ Baser r√©compenses sur m√©triques r√©elles (temps, erreurs, qualit√©)
- ‚úÖ P√©naliser strat√©gies inefficaces
- ‚úÖ R√©compenser strat√©gies efficaces
- ‚úÖ Ajuster poids selon impact r√©el

**Pattern:**
```typescript
// Syst√®me r√©compenses/p√©nalit√©s avanc√©
interface RewardSystem {
  metrics: RealMetrics;
  rewards: Reward[];
  penalties: Penalty[];
  weightAdjustment: WeightAdjustment;
}

class AdvancedRewardSystem {
  async calculateReward(
    action: AgentAction,
    result: ActionResult,
    context: Context
  ): Promise<Reward> {
    // 1. Collecter m√©triques r√©elles
    const metrics = await this.collectRealMetrics(action, result, context);
    
    // 2. Calculer r√©compense bas√©e sur m√©triques
    let reward = 0;
    
    // Exemple: try-catch ‚Üí withErrorHandling() r√©ussi
    if (action.type === 'replace-try-catch' && result.success) {
      reward += 10; // R√©compense base
      
      // Bonus selon impact
      if (metrics.errorRateReduction > 0.1) {
        reward += 5; // Bonus r√©duction erreurs
      }
      if (metrics.codeQualityImprovement > 0.2) {
        reward += 3; // Bonus qualit√© code
      }
      if (metrics.executionTimeReduction > 0.05) {
        reward += 2; // Bonus performance
      }
    }
    
    // Exemple: Migration big-bang √©choue
    if (action.type === 'migration-big-bang' && !result.success) {
      reward -= 5; // P√©nalit√© base
      
      // P√©nalit√© selon impact
      if (metrics.errorCount > 10) {
        reward -= 10; // P√©nalit√© erreurs nombreuses
      }
      if (metrics.rollbackRequired) {
        reward -= 5; // P√©nalit√© rollback
      }
    }
    
    // Exemple: Migration progressive r√©ussie
    if (action.type === 'migration-incremental' && result.success) {
      reward += 15; // R√©compense base (plus √©lev√©e que big-bang)
      
      // Bonus selon progression
      if (metrics.modulesMigrated > 0) {
        reward += metrics.modulesMigrated * 2; // Bonus par module
      }
      if (metrics.testsPassing) {
        reward += 5; // Bonus tests passent
      }
    }
    
    return {
      value: reward,
      metrics,
      action,
      result,
      timestamp: Date.now()
    };
  }
  
  private async collectRealMetrics(
    action: AgentAction,
    result: ActionResult,
    context: Context
  ): Promise<RealMetrics> {
    return {
      executionTime: result.executionTime,
      errorRate: result.errorCount / result.totalOperations,
      codeQuality: await this.measureCodeQuality(result, context),
      testCoverage: await this.measureTestCoverage(result, context),
      maintainability: await this.measureMaintainability(result, context),
      errorRateReduction: this.calculateErrorRateReduction(result, context),
      codeQualityImprovement: this.calculateCodeQualityImprovement(result, context),
      executionTimeReduction: this.calculateExecutionTimeReduction(result, context),
      errorCount: result.errorCount,
      rollbackRequired: result.rollbackRequired,
      modulesMigrated: result.modulesMigrated || 0,
      testsPassing: result.testsPassing || false
    };
  }
}
```

**Exemples R√©compenses/P√©nalit√©s:**

| Action | R√©sultat | R√©compense/P√©nalit√© | Raison |
|--------|----------|---------------------|--------|
| try-catch ‚Üí withErrorHandling() | ‚úÖ Succ√®s | +10 √† +20 | Standardisation r√©ussie |
| Migration big-bang | ‚ùå √âchec | -5 √† -20 | Approche risqu√©e √©choue |
| Migration progressive | ‚úÖ Succ√®s | +15 √† +30 | Approche s√ªre r√©ussit |
| Typage any ‚Üí Type sp√©cifique | ‚úÖ Succ√®s | +8 √† +15 | Am√©lioration qualit√© |
| Fichier monolithique ‚Üí Modules | ‚úÖ Succ√®s | +12 √† +25 | R√©duction dette technique |

## üß† Apprentissage Q-Learning

### Q-Table et Q-Function

**TOUJOURS:**
- ‚úÖ Maintenir Q-table pour √©tats/actions
- ‚úÖ Mettre √† jour Q-values selon r√©compenses
- ‚úÖ S√©lectionner actions avec meilleure Q-value
- ‚úÖ Explorer nouvelles actions p√©riodiquement

**Pattern:**
```typescript
// Q-Learning pour s√©lection techniques
interface QLearningSystem {
  qTable: QTable;
  learningRate: number;
  discountFactor: number;
  explorationRate: number;
}

class QLearningEngine {
  private qTable: Map<string, Map<string, number>> = new Map();
  private learningRate = 0.1;
  private discountFactor = 0.9;
  private explorationRate = 0.2; // 20% exploration, 80% exploitation
  
  async selectAction(
    state: AgentState,
    availableActions: AgentAction[],
    context: Context
  ): Promise<AgentAction> {
    // 1. Calculer Q-values pour chaque action
    const qValues = await Promise.all(
      availableActions.map(action => 
        this.getQValue(state, action, context)
      )
    );
    
    // 2. Exploration vs Exploitation
    if (Math.random() < this.explorationRate) {
      // Exploration: S√©lectionner action al√©atoire
      return availableActions[Math.floor(Math.random() * availableActions.length)];
    } else {
      // Exploitation: S√©lectionner action avec meilleure Q-value
      const maxQValue = Math.max(...qValues);
      const bestActionIndex = qValues.indexOf(maxQValue);
      return availableActions[bestActionIndex];
    }
  }
  
  async updateQValue(
    state: AgentState,
    action: AgentAction,
    reward: Reward,
    nextState: AgentState,
    context: Context
  ): Promise<void> {
    // 1. Obtenir Q-value actuelle
    const currentQ = await this.getQValue(state, action, context);
    
    // 2. Calculer Q-value maximale pour √©tat suivant
    const nextStateActions = await this.getAvailableActions(nextState, context);
    const nextQValues = await Promise.all(
      nextStateActions.map(a => this.getQValue(nextState, a, context))
    );
    const maxNextQ = nextQValues.length > 0 ? Math.max(...nextQValues) : 0;
    
    // 3. Mettre √† jour Q-value (formule Q-learning)
    const newQ = currentQ + this.learningRate * (
      reward.value + this.discountFactor * maxNextQ - currentQ
    );
    
    // 4. Sauvegarder Q-value
    await this.setQValue(state, action, newQ, context);
  }
  
  private async getQValue(
    state: AgentState,
    action: AgentAction,
    context: Context
  ): Promise<number> {
    const stateKey = this.getStateKey(state);
    const actionKey = this.getActionKey(action);
    
    if (!this.qTable.has(stateKey)) {
      this.qTable.set(stateKey, new Map());
    }
    
    const stateQTable = this.qTable.get(stateKey)!;
    return stateQTable.get(actionKey) || 0; // Q-value initiale: 0
  }
  
  private async setQValue(
    state: AgentState,
    action: AgentAction,
    qValue: number,
    context: Context
  ): Promise<void> {
    const stateKey = this.getStateKey(state);
    const actionKey = this.getActionKey(action);
    
    if (!this.qTable.has(stateKey)) {
      this.qTable.set(stateKey, new Map());
    }
    
    const stateQTable = this.qTable.get(stateKey)!;
    stateQTable.set(actionKey, qValue);
    
    // Sauvegarder Q-table
    await this.saveQTable(context);
  }
}
```

**Exemple Q-Table:**

| √âtat | Action | Q-Value | Apprentissage |
|------|--------|---------|---------------|
| "741 try-catch d√©tect√©s" | "Remplacer par withErrorHandling()" | 15.2 | ‚úÖ R√©ussi plusieurs fois |
| "Migration n√©cessaire" | "Migration big-bang" | -3.5 | ‚ùå √âchou√© plusieurs fois |
| "Migration n√©cessaire" | "Migration progressive" | 18.7 | ‚úÖ R√©ussi plusieurs fois |
| "933 any d√©tect√©s" | "Typer avec types sp√©cifiques" | 12.1 | ‚úÖ R√©ussi plusieurs fois |

## ‚öñÔ∏è Exploration vs Exploitation Optimis√©e

### Strat√©gie Epsilon-Greedy Adaptative

**TOUJOURS:**
- ‚úÖ Ajuster taux d'exploration selon contexte
- ‚úÖ Explorer plus en d√©but d'apprentissage
- ‚úÖ Exploiter plus apr√®s convergence
- ‚úÖ R√©explorer si performance d√©grade

**Pattern:**
```typescript
// Exploration vs Exploitation optimis√©e
class AdaptiveExplorationExploitation {
  private baseExplorationRate = 0.2;
  private minExplorationRate = 0.05;
  private maxExplorationRate = 0.5;
  private convergenceThreshold = 0.01; // Variation Q-value < 1%
  
  async calculateExplorationRate(
    learningProgress: LearningProgress,
    context: Context
  ): Promise<number> {
    // 1. Calculer progression apprentissage
    const progress = this.calculateLearningProgress(learningProgress, context);
    
    // 2. Ajuster selon convergence
    if (progress.converged) {
      // Converg√©: R√©duire exploration
      return Math.max(
        this.minExplorationRate,
        this.baseExplorationRate * (1 - progress.convergenceScore)
      );
    } else {
      // Pas converg√©: Maintenir exploration
      return this.baseExplorationRate;
    }
    
    // 3. R√©explorer si performance d√©grade
    if (progress.performanceDegrading) {
      return Math.min(
        this.maxExplorationRate,
        this.baseExplorationRate * 2
      );
    }
    
    return this.baseExplorationRate;
  }
  
  private calculateLearningProgress(
    progress: LearningProgress,
    context: Context
  ): LearningProgressAnalysis {
    // Analyser variation Q-values
    const qValueVariance = this.calculateQVariance(progress.qTable, context);
    
    // V√©rifier convergence
    const converged = qValueVariance < this.convergenceThreshold;
    
    // V√©rifier d√©gradation performance
    const performanceDegrading = progress.recentRewards.length > 10 &&
      this.calculateAverageReward(progress.recentRewards.slice(-10)) <
      this.calculateAverageReward(progress.recentRewards.slice(-20, -10));
    
    return {
      converged,
      convergenceScore: 1 - (qValueVariance / this.convergenceThreshold),
      performanceDegrading,
      qValueVariance
    };
  }
}
```

## üéØ Convergence vers Strat√©gies Optimales

### D√©tection Convergence et Optimisation

**TOUJOURS:**
- ‚úÖ D√©tecter convergence Q-values
- ‚úÖ Identifier strat√©gies optimales
- ‚úÖ Appliquer strat√©gies optimales
- ‚úÖ Continuer apprentissage m√™me apr√®s convergence

**Pattern:**
```typescript
// Convergence vers strat√©gies optimales
class OptimalStrategyConvergence {
  async detectConvergence(
    qTable: QTable,
    context: Context
  ): Promise<ConvergenceResult> {
    // 1. Analyser variation Q-values
    const variance = await this.calculateQVariance(qTable, context);
    
    // 2. Identifier strat√©gies optimales
    const optimalStrategies = await this.identifyOptimalStrategies(
      qTable,
      context
    );
    
    // 3. V√©rifier convergence
    const converged = variance < 0.01; // Variation < 1%
    
    return {
      converged,
      variance,
      optimalStrategies,
      confidence: this.calculateConfidence(optimalStrategies, context)
    };
  }
  
  private async identifyOptimalStrategies(
    qTable: QTable,
    context: Context
  ): Promise<OptimalStrategy[]> {
    const strategies: OptimalStrategy[] = [];
    
    // Pour chaque √©tat, identifier action avec meilleure Q-value
    for (const [stateKey, stateQTable] of qTable.entries()) {
      let maxQ = -Infinity;
      let bestAction: string | null = null;
      
      for (const [actionKey, qValue] of stateQTable.entries()) {
        if (qValue > maxQ) {
          maxQ = qValue;
          bestAction = actionKey;
        }
      }
      
      if (bestAction && maxQ > 10) { // Seuil Q-value > 10
        strategies.push({
          state: this.parseStateKey(stateKey),
          action: this.parseActionKey(bestAction),
          qValue: maxQ,
          confidence: this.calculateStrategyConfidence(maxQ, context)
        });
      }
    }
    
    return strategies.sort((a, b) => b.qValue - a.qValue);
  }
}
```

## üîÑ Workflow Apprentissage Renforcement Avanc√©

### Workflow Complet

1. **Observer √©tat** ‚Üí √âtat actuel agent/t√¢che
2. **S√©lectionner action** ‚Üí Q-learning (exploration/exploitation)
3. **Ex√©cuter action** ‚Üí Appliquer action s√©lectionn√©e
4. **Recevoir r√©compense** ‚Üí Calculer r√©compense/p√©nalit√©
5. **Mettre √† jour Q-value** ‚Üí Apprendre de l'exp√©rience
6. **D√©tecter convergence** ‚Üí Identifier strat√©gies optimales
7. **Appliquer strat√©gies optimales** ‚Üí Utiliser apprentissages

**Pattern:**
```typescript
// Workflow complet apprentissage renforcement avanc√©
class AdvancedReinforcementLearning {
  async executeLearningCycle(
    task: Task,
    context: Context
  ): Promise<LearningCycleResult> {
    // 1. Observer √©tat
    const state = await this.observeState(task, context);
    
    // 2. S√©lectionner action (Q-learning)
    const action = await this.qLearningEngine.selectAction(
      state,
      await this.getAvailableActions(state, context),
      context
    );
    
    // 3. Ex√©cuter action
    const result = await this.executeAction(action, task, context);
    
    // 4. Recevoir r√©compense
    const reward = await this.rewardSystem.calculateReward(
      action,
      result,
      context
    );
    
    // 5. Observer nouvel √©tat
    const nextState = await this.observeState(task, context);
    
    // 6. Mettre √† jour Q-value
    await this.qLearningEngine.updateQValue(
      state,
      action,
      reward,
      nextState,
      context
    );
    
    // 7. D√©tecter convergence
    const convergence = await this.convergenceEngine.detectConvergence(
      this.qLearningEngine.qTable,
      context
    );
    
    // 8. Appliquer strat√©gies optimales si converg√©
    if (convergence.converged) {
      await this.applyOptimalStrategies(convergence.optimalStrategies, context);
    }
    
    return {
      state,
      action,
      result,
      reward,
      nextState,
      convergence,
      improvements: this.calculateImprovements(reward, convergence, context)
    };
  }
}
```

## ‚ö†Ô∏è R√®gles Apprentissage Renforcement Avanc√©

### TOUJOURS:

- ‚úÖ Baser r√©compenses sur m√©triques r√©elles
- ‚úÖ Utiliser Q-learning pour s√©lection actions
- ‚úÖ √âquilibrer exploration vs exploitation
- ‚úÖ D√©tecter convergence vers strat√©gies optimales
- ‚úÖ Appliquer strat√©gies optimales identifi√©es
- ‚úÖ Continuer apprentissage m√™me apr√®s convergence
- ‚úÖ Sauvegarder Q-table pour r√©utilisation

### NE JAMAIS:

- ‚ùå Ignorer m√©triques r√©elles pour r√©compenses
- ‚ùå Ne pas utiliser Q-learning pour s√©lection
- ‚ùå Ignorer √©quilibre exploration/exploitation
- ‚ùå Ne pas d√©tecter convergence
- ‚ùå Oublier d'appliquer strat√©gies optimales

## üîó R√©f√©rences

### R√®gles Int√©gr√©es

- `@.cursor/rules/reinforcement-learning.md` - Apprentissage par renforcement de base
- `@.cursor/rules/self-evolution-engine.md` - Moteur d'auto-√©volution
- `@.cursor/rules/learning-memory.md` - M√©moire persistante des apprentissages

### Documentation Externe

- [Q-Learning Algorithm](https://en.wikipedia.org/wiki/Q-learning)
- [Exploration-Exploitation Trade-off](https://en.wikipedia.org/wiki/Multi-armed_bandit)

---

**Note:** Ce fichier d√©finit un syst√®me d'apprentissage par renforcement avanc√© avec Q-learning, exploration-exploitation optimis√©e et convergence vers strat√©gies optimales.

**Version:** 2.0.0  
**Derni√®re mise √† jour:** 2025-01-29

